{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Data Collection & Cleaning\n",
    "\n",
    "This notebook demonstrates the data collection and cleaning process for the CityPulse project.\n",
    "\n",
    "## Setup: Install Dependencies\n",
    "\n",
    "Run the cell below first to ensure all required packages are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required packages...\n",
      "Python: /Users/mukul/Desktop/chieac project/.venv/bin/python\n",
      "\n",
      "✓ requests - OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukul/Desktop/chieac project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pandas - OK\n",
      "✓ numpy - OK\n",
      "✓ vaderSentiment - OK\n",
      "✓ textblob - OK\n",
      "✓ plotly - OK\n",
      "✓ dash - OK\n",
      "✓ dash-bootstrap-components - OK\n",
      "✓ geopy - OK\n",
      "✓ tweepy - OK\n",
      "✓ dotenv - OK\n",
      "\n",
      "✓ All packages are installed! You can now run the data collection cells.\n"
     ]
    }
   ],
   "source": [
    "# Verify required packages are installed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "print(\"Checking required packages...\")\n",
    "print(f\"Python: {sys.executable}\\n\")\n",
    "\n",
    "# Test imports\n",
    "required_packages = {\n",
    "    \"requests\": \"requests\",\n",
    "    \"pandas\": \"pandas\", \n",
    "    \"numpy\": \"numpy\",\n",
    "    \"vaderSentiment\": \"vaderSentiment\",\n",
    "    \"textblob\": \"textblob\",\n",
    "    \"plotly\": \"plotly\",\n",
    "    \"dash\": \"dash\",\n",
    "    \"dash-bootstrap-components\": \"dash-bootstrap-components\",\n",
    "    \"geopy\": \"geopy\",\n",
    "    \"tweepy\": \"tweepy\",\n",
    "    \"python-dotenv\": \"dotenv\"\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for import_name, package_name in required_packages.items():\n",
    "    try:\n",
    "        if import_name == \"vaderSentiment\":\n",
    "            from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        elif import_name == \"dash-bootstrap-components\":\n",
    "            import dash_bootstrap_components\n",
    "        elif import_name == \"python-dotenv\":\n",
    "            import dotenv\n",
    "        else:\n",
    "            __import__(import_name)\n",
    "        print(f\"✓ {package_name} - OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"✗ {package_name} - MISSING\")\n",
    "        print(f\"  Error: {e}\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n✓ All packages are installed! You can now run the data collection cells.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some packages are missing!\")\n",
    "    print(\"Please run this command in your terminal:\")\n",
    "    print(\"  cd '/Users/mukul/Desktop/chieac project' && source .venv/bin/activate && pip install -r requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Run the data collection scripts to gather data from all sources.\n",
    "\n",
    "Each data source has its own cell below - run them individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete - ready for data collection\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import path configuration\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure src is in path\n",
    "project_root = os.path.abspath('..')\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Verify basic imports\n",
    "try:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    print(\"✓ Setup complete - ready for data collection\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"Please run the setup cell (cell 1) first!\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collect 311 Service Requests Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 10:22:52,819 - INFO - Starting 311 data collection\n",
      "2025-12-08 10:22:52,819 - INFO - Attempting to fetch 311 data with keyword filter...\n",
      "2025-12-08 10:22:52,819 - INFO - Fetching 311 data from 2025-09-09T00:00:00.000 to 2025-12-08T23:59:59.999\n",
      "2025-12-08 10:22:52,819 - INFO - Fetching batch: offset=0, limit=5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Collecting 311 Service Requests Data\n",
      "============================================================\n",
      "Source: Chicago Data Portal - Transit-related complaints\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 10:23:12,554 - ERROR - Error fetching data: 400 Client Error: Bad Request for url: https://data.cityofchicago.org/resource/v6vf-nfxy.json?%24limit=5000&%24offset=0&%24where=created_date+%3E%3D+%272025-09-09T00%3A00%3A00.000%27+AND+created_date+%3C%3D+%272025-12-08T23%3A59%3A59.999%27+AND+%28service_request_type+like+%27%25street%25%27+OR+service_request_type+like+%27%25light%25%27+OR+service_request_type+like+%27%25pothole%25%27+OR+service_request_type+like+%27%25traffic%25%27+OR+service_request_type+like+%27%25sidewalk%25%27+OR+service_request_type+like+%27%25alley%25%27%29&%24order=created_date+DESC\n",
      "2025-12-08 10:23:12,555 - WARNING - No records fetched\n",
      "2025-12-08 10:23:12,560 - INFO - No data with keyword filter. Trying without service type filter...\n",
      "2025-12-08 10:23:12,560 - INFO - Fetching 311 data from 2025-09-09T00:00:00.000 to 2025-12-08T23:59:59.999\n",
      "2025-12-08 10:23:12,561 - INFO - Fetching batch: offset=0, limit=5000\n",
      "2025-12-08 10:23:15,982 - INFO - Fetched 5000 records (total: 5000)\n",
      "2025-12-08 10:23:16,488 - INFO - Fetching batch: offset=5000, limit=5000\n",
      "2025-12-08 10:23:18,095 - INFO - Fetched 5000 records (total: 10000)\n",
      "2025-12-08 10:23:18,598 - INFO - Fetching batch: offset=10000, limit=5000\n",
      "2025-12-08 10:23:26,373 - INFO - Fetched 5000 records (total: 15000)\n",
      "2025-12-08 10:23:26,879 - INFO - Fetching batch: offset=15000, limit=5000\n",
      "2025-12-08 10:23:29,102 - INFO - Fetched 5000 records (total: 20000)\n",
      "2025-12-08 10:23:29,608 - INFO - Fetching batch: offset=20000, limit=5000\n",
      "2025-12-08 10:23:32,148 - INFO - Fetched 5000 records (total: 25000)\n",
      "2025-12-08 10:23:32,654 - INFO - Fetching batch: offset=25000, limit=5000\n",
      "2025-12-08 10:23:34,085 - INFO - Fetched 5000 records (total: 30000)\n",
      "2025-12-08 10:23:34,589 - INFO - Fetching batch: offset=30000, limit=5000\n",
      "2025-12-08 10:23:36,701 - INFO - Fetched 5000 records (total: 35000)\n",
      "2025-12-08 10:23:37,205 - INFO - Fetching batch: offset=35000, limit=5000\n",
      "2025-12-08 10:23:39,097 - INFO - Fetched 5000 records (total: 40000)\n",
      "2025-12-08 10:23:39,600 - INFO - Fetching batch: offset=40000, limit=5000\n",
      "2025-12-08 10:23:45,052 - INFO - Fetched 5000 records (total: 45000)\n",
      "2025-12-08 10:23:45,558 - INFO - Fetching batch: offset=45000, limit=5000\n",
      "2025-12-08 10:23:47,852 - INFO - Fetched 5000 records (total: 50000)\n",
      "2025-12-08 10:23:48,505 - INFO - Total records fetched: 50000\n",
      "2025-12-08 10:23:48,533 - INFO - Filtered to 50000 transit-related records\n",
      "2025-12-08 10:23:48,942 - INFO - Saved 50000 records to /Users/mukul/Desktop/chieac project/data/raw/311_raw.csv\n",
      "2025-12-08 10:23:48,942 - INFO - \n",
      "=== Data Summary ===\n",
      "2025-12-08 10:23:48,942 - INFO - Total records: 50000\n",
      "2025-12-08 10:23:48,951 - INFO - \n",
      "Date range: 2025-11-28T12:53:47.000 to 2025-12-08T04:25:22.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 311 data collection complete!\n"
     ]
    }
   ],
   "source": [
    "# Collect 311 Service Requests Data\n",
    "print(\"=\"*60)\n",
    "print(\"Collecting 311 Service Requests Data\")\n",
    "print(\"=\"*60)\n",
    "print(\"Source: Chicago Data Portal - Transit-related complaints\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from data_collection.collect_311_data import main as collect_311\n",
    "    collect_311()\n",
    "    print(\"\\n✓ 311 data collection complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error collecting 311 data: {e}\")\n",
    "    print(\"\\nNote: This might be due to:\")\n",
    "    print(\"- API query syntax issues\")\n",
    "    print(\"- Network connectivity\")\n",
    "    print(\"- API endpoint changes\")\n",
    "    print(\"\\nYou can continue with other data sources.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collect CTA Ridership Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 10:23:49,004 - INFO - Starting CTA ridership data collection\n",
      "2025-12-08 10:23:49,004 - INFO - Fetching CTA bus ridership data\n",
      "2025-12-08 10:23:49,005 - INFO - Fetching bus data: offset=0, limit=5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Collecting CTA Ridership Data\n",
      "============================================================\n",
      "Source: Chicago Data Portal - Bus and Train ridership\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 10:23:49,501 - INFO - Fetched 2563 bus records (total: 2563)\n",
      "2025-12-08 10:23:49,506 - INFO - Total bus records: 2563\n",
      "2025-12-08 10:23:49,508 - INFO - Fetching CTA train ridership data\n",
      "2025-12-08 10:23:49,508 - INFO - Trying train endpoint: https://data.cityofchicago.org/resource/5neh-572f.json\n",
      "2025-12-08 10:23:49,509 - INFO - Fetching train data: offset=0, limit=5000\n",
      "2025-12-08 10:23:49,974 - INFO - Fetched 3168 train records (total: 3168)\n",
      "2025-12-08 10:23:49,975 - INFO - Successfully fetched train data from https://data.cityofchicago.org/resource/5neh-572f.json\n",
      "2025-12-08 10:23:49,980 - INFO - Total train records: 3168\n",
      "2025-12-08 10:23:49,984 - INFO - Combined ridership data: 5731 total records\n",
      "2025-12-08 10:23:49,997 - INFO - Saved 5731 records to /Users/mukul/Desktop/chieac project/data/raw/cta_raw.csv\n",
      "2025-12-08 10:23:49,998 - INFO - \n",
      "=== Data Summary ===\n",
      "2025-12-08 10:23:49,998 - INFO - Total records: 5731\n",
      "2025-12-08 10:23:49,998 - INFO - \n",
      "By mode:\n",
      "2025-12-08 10:23:50,000 - INFO - mode\n",
      "train    3168\n",
      "bus      2563\n",
      "Name: count, dtype: int64\n",
      "2025-12-08 10:23:50,001 - INFO - \n",
      "Date range: 2025-09-09T00:00:00.000 to 2025-09-30T00:00:00.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ CTA data collection complete!\n"
     ]
    }
   ],
   "source": [
    "# Collect CTA Ridership Data (Bus & Train)\n",
    "print(\"=\"*60)\n",
    "print(\"Collecting CTA Ridership Data\")\n",
    "print(\"=\"*60)\n",
    "print(\"Source: Chicago Data Portal - Bus and Train ridership\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from data_collection.collect_cta_data import main as collect_cta\n",
    "    collect_cta()\n",
    "    print(\"\\n✓ CTA data collection complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error collecting CTA data: {e}\")\n",
    "    print(\"\\nNote: This might be due to:\")\n",
    "    print(\"- API endpoint changes\")\n",
    "    print(\"- Network connectivity\")\n",
    "    print(\"- Data availability for the date range\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Collect Twitter/X Data (Real Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 11:41:44,827 - INFO - Starting Twitter data collection using Twitter API v2\n",
      "2025-12-08 11:41:44,828 - INFO - Using Bearer Token authentication\n",
      "2025-12-08 11:41:44,830 - INFO - Twitter API client initialized successfully\n",
      "2025-12-08 11:41:44,831 - INFO - Searching for tweets: CTA (max: 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Collecting REAL Twitter/X Data\n",
      "============================================================\n",
      "Source: Twitter API v2 - Chicago-related hashtags\n",
      "Using Bearer Token from .env file\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 11:41:45,207 - INFO - Collected 21/500 tweets for CTA (request 1)\n",
      "2025-12-08 11:41:45,208 - INFO - No more pages available for CTA\n",
      "2025-12-08 11:41:45,208 - INFO - Collected 21 tweets for CTA\n",
      "2025-12-08 11:41:50,210 - INFO - Searching for tweets: ChicagoTransit (max: 500)\n",
      "2025-12-08 11:41:50,276 - WARNING - Rate limit exceeded. Sleeping for 895 seconds.\n",
      "2025-12-08 11:56:45,601 - INFO - Collected 4/500 tweets for ChicagoTransit (request 1)\n",
      "2025-12-08 11:56:45,603 - INFO - No more pages available for ChicagoTransit\n",
      "2025-12-08 11:56:45,604 - INFO - Collected 4 tweets for ChicagoTransit\n",
      "2025-12-08 11:56:45,612 - INFO - Processed 25 unique tweets\n",
      "2025-12-08 11:56:45,618 - INFO - Saved 25 tweets to /Users/mukul/Desktop/chieac project/data/raw/tweets_raw.csv\n",
      "2025-12-08 11:56:45,618 - INFO - \n",
      "=== Data Summary ===\n",
      "2025-12-08 11:56:45,619 - INFO - Total tweets: 25\n",
      "2025-12-08 11:56:45,620 - INFO - Date range: 2025-12-07T18:02:35+00:00 to 2025-12-08T17:12:33+00:00\n",
      "2025-12-08 11:56:45,620 - INFO - Hashtags collected: CTA, ChicagoTransit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Real Twitter data collected successfully!\n"
     ]
    }
   ],
   "source": [
    "# Collect REAL Twitter/X Data using Twitter API v2\n",
    "# Setup: Ensure src is in path\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath('..')\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Collecting REAL Twitter/X Data\")\n",
    "print(\"=\"*60)\n",
    "print(\"Source: Twitter API v2 - Chicago-related hashtags\")\n",
    "print(\"Using Bearer Token from .env file\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from data_collection.collect_tweets_tweepy import main as collect_tweets_tweepy\n",
    "    collect_tweets_tweepy()\n",
    "    print(\"\\n✓ Real Twitter data collected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Error collecting real Twitter data: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check that .env file exists with TWITTER_BEARER_TOKEN\")\n",
    "    print(\"2. Verify your Bearer Token is correct\")\n",
    "    print(\"3. Check Twitter API rate limits (free tier: 10k tweets/day)\")\n",
    "    print(\"4. Wait a few minutes if you hit rate limits - script will auto-retry\")\n",
    "    print(\"\\nNote: Twitter API free tier only allows last 7 days of tweets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Clean and preprocess all collected datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 10:45:33,597 - INFO - Starting data cleaning process\n",
      "/Users/mukul/Desktop/chieac project/src/data_cleaning/clean_data.py:277: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_311 = pd.read_csv(PROJECT_ROOT / \"data\" / \"raw\" / \"311_raw.csv\")\n",
      "2025-12-08 10:45:33,816 - INFO - Loaded 311 data: 50000 records\n",
      "2025-12-08 10:45:33,821 - INFO - Loaded CTA data: 5731 records\n",
      "2025-12-08 10:45:33,822 - INFO - Loaded tweet data: 23 records\n",
      "2025-12-08 10:45:33,822 - INFO - Cleaning 311 data\n",
      "2025-12-08 10:45:33,842 - INFO - Normalized created_date to datetime format\n",
      "2025-12-08 10:45:33,851 - INFO - Normalized closed_date to datetime format\n",
      "2025-12-08 10:45:33,934 - INFO - Dropped 0 rows with all missing values\n",
      "2025-12-08 10:45:33,940 - INFO - Location normalization: Basic structure added. Full geocoding requires external API.\n",
      "2025-12-08 10:45:33,941 - INFO - Cleaned 311 data: 50000 records\n",
      "2025-12-08 10:45:34,443 - INFO - Saved cleaned 311 data: 50000 records\n",
      "2025-12-08 10:45:34,443 - INFO - Cleaning CTA ridership data\n",
      "2025-12-08 10:45:34,445 - INFO - Normalized date to datetime format\n",
      "/Users/mukul/Desktop/chieac project/src/data_cleaning/clean_data.py:44: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
      "2025-12-08 10:45:34,448 - INFO - Normalized daytype to datetime format\n",
      "2025-12-08 10:45:34,453 - INFO - Filled missing values with defaults\n",
      "2025-12-08 10:45:34,453 - INFO - Cleaned CTA data: 5731 records\n",
      "2025-12-08 10:45:34,462 - INFO - Saved cleaned CTA data: 5731 records\n",
      "2025-12-08 10:45:34,462 - INFO - Cleaning tweet data\n",
      "2025-12-08 10:45:34,463 - INFO - Normalized date to datetime format\n",
      "2025-12-08 10:45:34,464 - INFO - Dropped 0 rows with all missing values\n",
      "2025-12-08 10:45:34,464 - INFO - Cleaned tweet data: 23 records\n",
      "2025-12-08 10:45:34,466 - INFO - Saved cleaned tweet data: 23 records\n",
      "2025-12-08 10:45:34,466 - INFO - Data cleaning complete\n"
     ]
    }
   ],
   "source": [
    "# Run data cleaning\n",
    "from data_cleaning.clean_data import main as clean_data\n",
    "clean_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Explore the cleaned datasets to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/m1jgpx_53fg_5_5sqk_64srh0000gn/T/ipykernel_57008/1783719673.py:10: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_311 = pd.read_csv('../data/cleaned/311_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Cleaned Datasets ===\n",
      "\n",
      "=== 311 Data Summary ===\n",
      "Shape: (50000, 37)\n",
      "Columns: ['sr_number', 'sr_type', 'sr_short_code', 'created_department', 'owner_department', 'status', 'origin', 'created_date', 'last_modified_date', 'closed_date', 'street_address', 'city', 'state', 'zip_code', 'street_number', 'street_direction', 'street_name', 'street_type', 'duplicate', 'legacy_record', 'created_hour', 'created_day_of_week', 'created_month', 'x_coordinate', 'y_coordinate', 'latitude', 'longitude', 'location', 'community_area', 'ward', 'electrical_district', 'electricity_grid', 'police_sector', 'police_district', 'police_beat', 'precinct', 'parent_sr_number']\n",
      "\n",
      "First few rows:\n",
      "       sr_number                       sr_type sr_short_code  \\\n",
      "0  SR25-02248186     311 INFORMATION ONLY CALL        311IOC   \n",
      "1  SR25-02248185     311 INFORMATION ONLY CALL        311IOC   \n",
      "2  SR25-02248184     311 INFORMATION ONLY CALL        311IOC   \n",
      "3  SR25-02248183  Ice and Snow Removal Request           SDO   \n",
      "4  SR25-02248182     311 INFORMATION ONLY CALL        311IOC   \n",
      "\n",
      "  created_department        owner_department     status      origin  \\\n",
      "0  311 City Services       311 City Services  Completed  Phone Call   \n",
      "1  311 City Services       311 City Services  Completed  Phone Call   \n",
      "2  311 City Services       311 City Services  Completed  Phone Call   \n",
      "3  311 City Services  Streets and Sanitation       Open  Phone Call   \n",
      "4  311 City Services       311 City Services  Completed  Phone Call   \n",
      "\n",
      "          created_date       last_modified_date          closed_date  ...  \\\n",
      "0  2025-12-08 04:25:22  2025-12-08T04:25:23.000  2025-12-08 04:25:23  ...   \n",
      "1  2025-12-08 04:24:33  2025-12-08T04:24:33.000  2025-12-08 04:24:33  ...   \n",
      "2  2025-12-08 04:21:40  2025-12-08T04:21:40.000  2025-12-08 04:21:40  ...   \n",
      "3  2025-12-08 04:20:32  2025-12-08T04:20:34.000                  NaN  ...   \n",
      "4  2025-12-08 04:19:53  2025-12-08T04:19:55.000  2025-12-08 04:19:54  ...   \n",
      "\n",
      "                                            location community_area  ward  \\\n",
      "0                                                NaN            NaN   NaN   \n",
      "1                                                NaN            NaN   NaN   \n",
      "2  {'latitude': '41.911227000941', 'longitude': '...            NaN   NaN   \n",
      "3  {'latitude': '41.911227000941', 'longitude': '...           25.0  29.0   \n",
      "4                                                NaN            NaN   NaN   \n",
      "\n",
      "   electrical_district electricity_grid police_sector police_district  \\\n",
      "0                  NaN              NaN           NaN             NaN   \n",
      "1                  NaN              NaN           NaN             NaN   \n",
      "2                  NaN              NaN           NaN             NaN   \n",
      "3                  6.0             B018           3.0            25.0   \n",
      "4                  NaN              NaN           NaN             NaN   \n",
      "\n",
      "  police_beat  precinct  parent_sr_number  \n",
      "0         NaN       NaN               NaN  \n",
      "1         NaN       NaN               NaN  \n",
      "2         NaN       NaN               NaN  \n",
      "3      2531.0      17.0               NaN  \n",
      "4         NaN       NaN               NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "=== CTA Data Summary ===\n",
      "Shape: (5731, 7)\n",
      "Columns: ['route', 'date', 'daytype', 'rides', 'mode', 'station_id', 'stationname']\n",
      "\n",
      "First few rows:\n",
      "  route        date  daytype  rides mode  station_id stationname\n",
      "0    X9  2025-09-30      NaN   6171  bus         0.0         NaN\n",
      "1   X49  2025-09-30      NaN   5165  bus         0.0         NaN\n",
      "2    X4  2025-09-30      NaN   3127  bus         0.0         NaN\n",
      "3    N5  2025-09-30      NaN    125  bus         0.0         NaN\n",
      "4   J14  2025-09-30      NaN   8290  bus         0.0         NaN\n",
      "\n",
      "=== Tweet Data Summary ===\n",
      "Shape: (23, 12)\n",
      "Columns: ['tweet_id', 'url', 'date', 'content', 'user', 'retweet_count', 'like_count', 'reply_count', 'quote_count', 'hashtags', 'coordinates', 'place']\n",
      "\n",
      "First few rows:\n",
      "              tweet_id                                                url  \\\n",
      "0  1998037426648809476  https://twitter.com/MrTopStep/status/199803742...   \n",
      "1  1998019304814481740  https://twitter.com/MyTransit_CH/status/199801...   \n",
      "2  1997993908127555952  https://twitter.com/MyTransit_CH/status/199799...   \n",
      "3  1997981293569548334  https://twitter.com/MyTransit_CH/status/199798...   \n",
      "4  1997974675020751260  https://twitter.com/McGinnisLoy/status/1997974...   \n",
      "\n",
      "                        date  \\\n",
      "0  2025-12-08 14:30:18+00:00   \n",
      "1  2025-12-08 13:18:18+00:00   \n",
      "2  2025-12-08 11:37:23+00:00   \n",
      "3  2025-12-08 10:47:15+00:00   \n",
      "4  2025-12-08 10:20:57+00:00   \n",
      "\n",
      "                                             content          user  \\\n",
      "0  $GS #CTA #Flows \\n\\n$YM $MYM $DIA $DJD $DJIA $...     MrTopStep   \n",
      "1  ‘This should have been avoided’: Late warnings...  MyTransit_CH   \n",
      "2  Exec at construction firm tied to Palumbo fami...  MyTransit_CH   \n",
      "3  Chicago CTA holding meeting for feedback on bu...  MyTransit_CH   \n",
      "4  Personal #Tax Senior req'd (#CTA or #ATT Quali...   McGinnisLoy   \n",
      "\n",
      "   retweet_count  like_count  reply_count  quote_count  \\\n",
      "0              0           0            2            0   \n",
      "1              0           0            0            0   \n",
      "2              0           0            0            0   \n",
      "3              0           0            0            0   \n",
      "4              0           0            0            0   \n",
      "\n",
      "                                         hashtags  coordinates  place  \n",
      "0                                    #CTA, #Flows          NaN    NaN  \n",
      "1  #Chicago, #Transit, #CTA, #Metra, #ChicagoNews          NaN    NaN  \n",
      "2  #Chicago, #Transit, #CTA, #Metra, #ChicagoNews          NaN    NaN  \n",
      "3  #Chicago, #Transit, #CTA, #Metra, #ChicagoNews          NaN    NaN  \n",
      "4     #Tax, #ATT, #Bracknell, #Reading, #taxjobs.          NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load cleaned datasets (handle missing files gracefully)\n",
    "print(\"=== Loading Cleaned Datasets ===\\n\")\n",
    "\n",
    "# 311 Data\n",
    "if os.path.exists('../data/cleaned/311_data.csv'):\n",
    "    df_311 = pd.read_csv('../data/cleaned/311_data.csv')\n",
    "    print(\"=== 311 Data Summary ===\")\n",
    "    print(f\"Shape: {df_311.shape}\")\n",
    "    print(f\"Columns: {list(df_311.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_311.head())\n",
    "    print()\n",
    "else:\n",
    "    print(\"⚠ 311_data.csv not found - run data collection and cleaning first\")\n",
    "    df_311 = None\n",
    "\n",
    "# CTA Data\n",
    "if os.path.exists('../data/cleaned/cta_ridership.csv'):\n",
    "    df_cta = pd.read_csv('../data/cleaned/cta_ridership.csv')\n",
    "    print(\"=== CTA Data Summary ===\")\n",
    "    print(f\"Shape: {df_cta.shape}\")\n",
    "    print(f\"Columns: {list(df_cta.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_cta.head())\n",
    "    print()\n",
    "else:\n",
    "    print(\"⚠ cta_ridership.csv not found - run data collection and cleaning first\")\n",
    "    df_cta = None\n",
    "\n",
    "# Tweet Data\n",
    "if os.path.exists('../data/cleaned/tweets.csv'):\n",
    "    df_tweets = pd.read_csv('../data/cleaned/tweets.csv')\n",
    "    print(\"=== Tweet Data Summary ===\")\n",
    "    print(f\"Shape: {df_tweets.shape}\")\n",
    "    print(f\"Columns: {list(df_tweets.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_tweets.head())\n",
    "else:\n",
    "    print(\"⚠ tweets.csv not found - run data collection and cleaning first\")\n",
    "    df_tweets = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Perform data quality checks on cleaned datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values ===\n",
      "\n",
      "311 Data:\n",
      "sr_number                  0\n",
      "sr_type                    0\n",
      "sr_short_code              0\n",
      "created_department     21431\n",
      "owner_department           0\n",
      "status                     0\n",
      "origin                     0\n",
      "created_date               0\n",
      "last_modified_date         0\n",
      "closed_date            10035\n",
      "street_address            39\n",
      "city                    5734\n",
      "state                   5734\n",
      "zip_code                7181\n",
      "street_number             60\n",
      "street_direction          70\n",
      "street_name               39\n",
      "street_type              412\n",
      "duplicate                  0\n",
      "legacy_record              0\n",
      "created_hour               0\n",
      "created_day_of_week        0\n",
      "created_month              0\n",
      "x_coordinate              77\n",
      "y_coordinate              77\n",
      "latitude                  77\n",
      "longitude                 77\n",
      "location                  77\n",
      "community_area            96\n",
      "ward                     100\n",
      "electrical_district     8671\n",
      "electricity_grid        8685\n",
      "police_sector             97\n",
      "police_district           97\n",
      "police_beat               97\n",
      "precinct                 113\n",
      "parent_sr_number       48492\n",
      "dtype: int64\n",
      "\n",
      "CTA Data:\n",
      "route             0\n",
      "date              0\n",
      "daytype        5731\n",
      "rides             0\n",
      "mode              0\n",
      "station_id        0\n",
      "stationname    2563\n",
      "dtype: int64\n",
      "\n",
      "Tweet Data:\n",
      "tweet_id          0\n",
      "url               0\n",
      "date              0\n",
      "content           0\n",
      "user              0\n",
      "retweet_count     0\n",
      "like_count        0\n",
      "reply_count       0\n",
      "quote_count       0\n",
      "hashtags          0\n",
      "coordinates      23\n",
      "place            23\n",
      "dtype: int64\n",
      "\n",
      "=== Date Ranges ===\n",
      "311 Data: 2025-11-28 12:53:47 to 2025-12-08 04:25:22\n",
      "CTA Data: 2025-09-09 to 2025-09-30\n",
      "Tweet Data: 2025-12-07 18:02:35+00:00 to 2025-12-08 14:30:18+00:00\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"=== Missing Values ===\\n\")\n",
    "\n",
    "if df_311 is not None and not df_311.empty:\n",
    "    print(\"311 Data:\")\n",
    "    print(df_311.isnull().sum())\n",
    "    print()\n",
    "else:\n",
    "    print(\"311 Data: Not available\\n\")\n",
    "\n",
    "if df_cta is not None and not df_cta.empty:\n",
    "    print(\"CTA Data:\")\n",
    "    print(df_cta.isnull().sum())\n",
    "    print()\n",
    "else:\n",
    "    print(\"CTA Data: Not available\\n\")\n",
    "\n",
    "if df_tweets is not None and not df_tweets.empty:\n",
    "    print(\"Tweet Data:\")\n",
    "    print(df_tweets.isnull().sum())\n",
    "    print()\n",
    "else:\n",
    "    print(\"Tweet Data: Not available\\n\")\n",
    "\n",
    "# Check date ranges\n",
    "print(\"=== Date Ranges ===\")\n",
    "if df_311 is not None and not df_311.empty and 'created_date' in df_311.columns:\n",
    "    print(f\"311 Data: {df_311['created_date'].min()} to {df_311['created_date'].max()}\")\n",
    "elif df_311 is not None:\n",
    "    print(\"311 Data: Available but no date column found\")\n",
    "else:\n",
    "    print(\"311 Data: Not available\")\n",
    "\n",
    "if df_cta is not None and not df_cta.empty and 'date' in df_cta.columns:\n",
    "    print(f\"CTA Data: {df_cta['date'].min()} to {df_cta['date'].max()}\")\n",
    "elif df_cta is not None:\n",
    "    print(\"CTA Data: Available but no date column found\")\n",
    "else:\n",
    "    print(\"CTA Data: Not available\")\n",
    "\n",
    "if df_tweets is not None and not df_tweets.empty and 'date' in df_tweets.columns:\n",
    "    print(f\"Tweet Data: {df_tweets['date'].min()} to {df_tweets['date'].max()}\")\n",
    "elif df_tweets is not None:\n",
    "    print(\"Tweet Data: Available but no date column found\")\n",
    "else:\n",
    "    print(\"Tweet Data: Not available\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
